>### Courses in this Specialization

1. Neural Networks and Deep Learning.
2. Improving Deep Neural Networks:Hyperparameter tuning,Regularization and Optimization
3. Structuring your Machine Learning project
4. Convolutional Neural Networks
5. Natural Language Processing:Building sequence models


**1.Neural Networks and Deep Learning.**

你将知道如何建立一个深度神经网络并且使它奏效

第一门课
* 有四个星期的材料
* 每一周的结尾都有十多个多选题

Outline of this Course
Week 1:Introducetion
Week 2:Basics of Neural Network programming
Week 3:One hidden layer Neural Networks
Week 4:Deep Neural Networks

**Week 2:Basics of Neural Network programming**

逻辑回归是一个用于二分类的算法，可以看作是一个非常小的神经网络，在二分类问题中，我们的目标就是习得一个分类器，以特征向量x作为输入，然后预测输出结果为1还是0，标签（label）只能为0或1，m为样本数

损失函数(lost function)：衡量在单个训练样本上的表现
1.  $L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^2$ (弃用)
2. $L(\hat{y},y) =-(y \log\hat{y}+(1-y)\log(1-\hat{y}))$

代价函数(cost function)：衡量在全部训练样本上的表现
$J(w,b)=\frac{1}{m}\sum\limits_{i=1}^mL(\hat{y}^{(i)},y^{(i)})=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})]$

参数更新操作：
$w:=w-\alpha \frac{\mathrm{d}J(w,b)}{\mathrm {d} w}$
$b:=b-\alpha \frac{\mathrm{d}J(w,b)}{\mathrm db}$
$\alpha$学习率可以控制我们在每一次迭代或者梯度下降法中步长大小
$\mathrm d$ 单个变量，$\partial$多个变量？
